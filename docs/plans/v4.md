Shriek v4
---
# Summary - partitioning via consistent hashing
Starting in this version, each Shriek node will no longer be a full replica of the entire
data set. Instead, the cluster will generate a fixed-size hash ring on startup and allocate
partitions to nodes as they join and leave the cluster.

## Ring hash
On cluster startup, the initial node generates the hash ring based on the requested number
of partitions, via `create keyspace <partition count=P> <preference list size=L>`. This partition
count is of fixed size for the duration of the cluster. The hash ring is based on [Jenkins hash](https://en.wikipedia.org/wiki/Jenkins_hash_function), which is a 32-bit hash; as such there
can be no more than 2^32 partitions, with a suggested upper limit of 1000 for small clusters.

When a key is set in Shriek, the partition of the ring to which it is assigned is determined by the Jenkins
hash value of the key. The message with the `SET` information is then routed to the correct node, and the
reply is routed back through the coordinator to the client.

## Partition preference lists
Prior to this version, every key in Shriek belonged to a single partition and
all writes were propagated to every node in the cluster. With the addition of the
ring hash, each node is responsible for a subset of the available partitions.

The initial node in the cluster is responsible for all partitions. As each node is added
to the cluster, it is assigned partitions from the ring. While there are N nodes in the cluster,
if N <= L, every node is a full replica. Once N > L, some partitions will appear some nodes and not others.


## Partition distribution

Secondly, suppose that instead of each node being responsible for keys up to the next node in the ring,
we replicated keys to the following K nodes - for example, if the order on the ring is node n1, n2, n3, n4
and K = 2, then if "cat = 1" is written to n1, it must also be written to n2 and n3.

This was written while trying to figure out ring hashing for Shriek (https://github.com/jsgoller1/shriek) based
on Dynamo's ring hash (specifically around preference lists).
---

Rebalancing plan:

Suppose we have a 4-partition ring, with partition ids 0, .25, .5, and .75.
When we add a key to our ring, the owner is determined by "sliding forward"
to the nearest node greater than or equal to the key - if we were to insert
"cat=foo" and hash(cat) = .73, then "cat=foo" would belong to node id .75,
as it is the first node whose id comes after the hash in the ring.

Suppose we wanted to add a new partition with id .85 (and that there is no
replication of keys; each key belongs to exactly one node). To rebalance the
keys, we would need to:
- find the nearest node (0) after our new node's id.
- obtain all keys in that node (e.g. copy and delete from it) whose hashes
are less than our new value (.85 and below)

When we have a replication strategy akin to Dynamo's where multiple
nodes own a single key, rebalancing also has to account for keys
that would be owned by a different node but are replicated to our new
node; if our 4-partition ring had a replication factor of 3, then node .85
would be responsible for a key that hashes to .24. Our new rebalancing
strategy to account for this is:

- Copy all keys from the nearest node (0) to the new node (.85)
- For each key k:
  - if .85 is on k's preference list, delete it from the node that was
  previously last on k's preference list
  - if .85 is not on the preference list, delete the key from .85.


Example:
Replication factor: 2
Keys: .20, .25, .55, .75, .90
Nodes: 0, .25, .5, .75

If we had a replication factor of 2, the key distribution would be:
(0): .55, .75, .90
(.25): .20, .25, .90
(.5): .20, .25,
(.75): .55, .75

If we add a new node (.85), then the new key distribution should be:
n1 (0): .90
n2 (.25): .20, .25, .90
n3 (.5): .20, .25,
n4 (.75): .55, .75
n5 (.85): .55, .75

## Assumptions
- The cluster's partition count will not be adjusted after creation (this is not desired
for real world use).
