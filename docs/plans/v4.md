Shriek v4
---
# Summary - partitioning via consistent hashing
Starting in this version, each Shriek node will no longer be a full replica of the entire
data set. Instead, the cluster will adhere to a partitioning strategy based on consistent hashing,
similar to the one described in the Dynamo paper.

enerate a fixed-size hash ring on startup and allocate
partitions to nodes as they join and leave the cluster.

## Ring hash
On cluster startup, the initial node generates the hash ring based on the requested number
of partitions, via `create keyspace <partition count=P> <preference list size=L>`. This partition
count is of fixed size for the duration of the cluster. The hash ring is based on [Jenkins hash](https://en.wikipedia.org/wiki/Jenkins_hash_function),
which is a 32-bit hash; as such there can be no more than 2^32 partitions, with a suggested upper limit of 1000 for small clusters.

When a key is set in Shriek, the partition of the ring to which it is assigned is determined by the Jenkins
hash value of the key. The message with the `SET` information is then routed to the correct node, and the
reply is routed back through the coordinator to the client.

## Partition preference lists
Prior to this version, every key in Shriek belonged to the same partition and
all writes were propagated to every node in the cluster. Starting in v4, each node is responsible
for a subset of the available partitions following the Dynamo paper's "preference list" idea -
when a key is to be added to Shriek, it is hashed, and the nearest node on the ring greater than
the hash is deemed the owner. Additionally, L many following nodes on the ring are also given ownership
of the key. In this version, nodes that are not on the preference list for a key do not own it,
and should not be contacted for reads or writes.

Suppose we have 4 nodes in our ring, with partition ids 0,
.25, .5, and .75 and a replication factor of 2. When we add a key to our ring, the owner is determined
by "sliding forward" to the nearest node greater than or equal to the key -
if we were to insert "cat=foo" and hash(cat) = .73, then "cat=foo"
would belong to node id .75, as it is the first node whose id comes after the hash in the ring, and
0, as it is the next node following .75 on the ring.

## Preference lists reads and writes
Starting in this version, `GET` and `SET` messages are coordinated to nodes on the preference list.
Every node in the preference list is queried on both reads and writes. Failures on either are not
reported to the client; for either a write or a read to succeed, at most one responsible node must
reply (this will be corrected in a later version implementing sloppy quorums and hinted handoff).

## Partition rebalancing (joins)
When a new node is added to the ring, the partitions must be rebalanced if the replication
factor is greater than 1. Suppose we wanted to add a new node with id .85 to the above ring.
If the replication factor were 1 instead of 2, the algorithm for rebalancing would be simple:
- find the nearest node (0) after our new node's id.
- obtain all keys in that node (e.g. copy and delete from it) whose hashes
are less than our new value (.85 and below)

However, because the replication factor is 2, we must account for the fact that other nodes
will also own the keys that 0 owns. The correct rebalancing algorithm is as follows:
- Copy all keys from the nearest node (0) to the new node (.85)
- For each key k:
  - if .85 is on k's preference list, delete it from the node that was
  previously last on k's preference list
  - if .85 is not on the preference list, delete the key from .85.

Example:
Replication factor: 2
Keys: .20, .25, .55, .75, .90
Nodes: 0, .25, .5, .75

The key distribution would be:
(0): .55, .75, .90
(.25): .20, .25, .90
(.5): .20, .25,
(.75): .55, .75

If we add a new node (.85), then the new key distribution should be:
n1 (0): .90
n2 (.25): .20, .25, .90
n3 (.5): .20, .25,
n4 (.75): .55, .75
n5 (.85): .55, .75

## Partition rebalancing (leaves)
A node can only be manually removed from the keyspace; in this version of Shriek, faults
are ignored and do not trigger any kind of automatic failover. A node can leave the cluster by issuing
a `LEAVE` message to any other node in the cluster. When a `LEAVE` occurs, it is broadcast to all other
nodes in the cluster. Each node then removes it from the ring. The node should then set issue a `SET`
on each key before it shuts down.

Example:
Replication factor: 3
Keys: .20, .25, .55, .75, .90
Nodes: 0, .20, .40, .60, .80

The key distribution would be:
0: .55, .75, .90
.20: .20, .75, .90
.40: .20, .25, .90
.60: .20, .25, .55,
.80: .25, .55, .75,

If a node (say .60) drops out, then the new key distribution should be:
0: .25, .55, .75, .90
.20: .20, .55, .75, .90
.40: .20, .25, .90
.80: .20, .25, .55, .75
