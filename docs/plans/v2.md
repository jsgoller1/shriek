Shriek v2
---
# Summary - Distributed storage with O(1) routing
Starting in v2, Shriek becomes a multi-node system with full replication.
In this version, the hash ring technically exists, but only has one
partition (of which every node is a member). Each Shriek node is aware of
every other node in the cluster. Writes are propagated to every node in
the cluster, but reads only occur against the node queried.

### Creating a cluster
A Shriek cluster is created starting with one node; a client connects to the node
and executes `create keyspace <partition count>`. In this version, all clusters
have only one partition, and the provided count is ignored.

### Adding and removing nodes from the cluster
A Shriek node can be added to the cluster with `join <port:ip>`. The new node will connect
to the remote node at the given port/ip and announce it is joining; the remote node will add
the new node to the hash ring (in which there is only one partition, owned by every node)
and return the updated hash ring to the new node. The remote node will then inform the rest
of the cluster. The new node will then know the addresses of each other node in the cluster.

A Shriek node can only manually be removed from the cluster; if a client connects to a node
and executes `quit cluster`, the node will broadcast to every other node that it is quitting,
and then shut down.

### Reads and writes
When the client sends a `SET` or `GET` message, the first node in the cluster to receive it is
called the "coordinator", as in the Dynamo paper. In this version, `GET`s are handled solely by the
coordinator and returned to the client. When a `SET` occurs, it is synchronously handled
by the coordinator and at least one other node in the cluster before the client receives an `OK`.
Once the `OK` is sent to the client, the write is then propagated to every other node in the cluster.
If the coordinator cannot reach any other nodes in the cluster, the write is declared failed and the
client is informed.

### Assumptions
In this version, it is assumed that:
- the partition count will not be resized after the cluster is created
- new nodes always join and exit the cluster successfully; every existing node
correctly learns about the new node, and the new node has a perfect
map of the cluster which includes itself.
