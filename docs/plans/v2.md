Shriek, roadmap to v2
---
## Statement (of the problem)
Per direction from the Bradfield assignments, the next things we need to support
for our K/V projects are:
- Multiple nodes, either in a leader or leaderless topology
- Data replication; losing a single node should not cause permanent data loss
- Data partitioning; each node should not need to be a full replica
- Disk serialization; we should be able to write data in memory to an on-disk format and vice versa
- A protocol that supports all of the above, plus getting / setting gets

## Understand / Plan
To facilitate the above needs, Shriek v2 will implement some of [Dynamo's](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf) features.
We will presume that the network Shriek operates in is _generally_ reliable (nodes may
occasionally become unreachable, but not to the extent that each node is unable to
reach every other node in its preference list).

### v1.1 - Consistent hashing
Shriek will used a ring-based consistent hash to ensure that nodes leaving and joining
the cluster do not cause too much data transfer. Every key in the keyspace
will map to a value between 0 and 2^32 using [Jenkins hash](https://en.wikipedia.org/wiki/Jenkins_hash_function).
When a key is set in Shriek, the partition of the ring to which it is assigned is determined by the Jenkins
hash value of the key.

In v1.1, Shriek clusters will not allow for the partition count to be resized once
created (despite this being desireable for real-world use). A cluster will be
created starting with a single node, to which a `create keyspace <partitions count>` command is
issued to determine the number of partitions. There can be no more than 2^32 partitions.

A Shriek node can be added to the cluster with `join <port:ip>`. The new node will connect
to the remote node at the given port/ip and announce it is joining; the remote node will add
the new node to the hash ring and return the updated hash ring to the new node. The remote
node will then inform the rest of the cluster
The new node will then know the addresses of each other node in the cluster.

In v1.1, it is assumed that:
- the partition count will not be resized after the cluster is created
- new joins always join the cluster successfully; every existing node correctly learns about the new node,
and the new node has a perfect map of the cluster which includes itself

### Lamport versioning
When a read occurs from a Shriek node, the top n healthy nodes that own the
partition associated with that key are queried. In a normal scenario, all of them are the same
and the result is returned to the user. If the keys differ, conflict resolution occurs
based on object specific Lamport clocks.

### Routing
Each Shriek node is aware of every other node in the cluster and which partitions that
node is responsible for. If a node receives a request for a key that is not in any partition
it manages, it simply forwards the request to the correct node and sends the result back to the client.

### Sloppy quorum and hinted handoff


## Execute
Versioned releases will be created for each individual version above.

## Review
Forthcoming

---

- routing / awareness of nodes joining and leaving cluster
