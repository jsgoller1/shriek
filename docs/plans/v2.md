Shriek v2
---
# Summary - Distributed storage with O(1) routing
Starting in v2, Shriek becomes a multi-node system with full replication.
Each Shriek node is aware of every other node in the cluster. Writes are
propagated to every node in the cluster, but reads only occur against the node queried.

### Creating a cluster
A Shriek cluster is created starting with one node; a client connects to the node
and executes `create keyspace -n <name> -r <replication factor>`. In this version, the replication
factor is always set to N for N nodes in the cluster, and the provided value is ignored. Only
one keyspace is allowed per machine.

Once a keyspace is created, each node in the cluster keeps a list of nodes in the cluster, mapping
their IDs to the address/port they can be reached at.

### Adding and removing nodes from the cluster
A Shriek node can be added to the cluster with `join <port:ip>`. The new node will connect
to the remote node at the given port/ip and announce it is joining; the remote node will add
the new node to the cluster by assigning it a valid ID, and return the id list to the new node.
The remote node will then inform the rest of the cluster. The new node will then know the
addresses of each other node in the cluster.

A Shriek node can only manually be removed from the cluster; if a client connects to a node
and executes `quit cluster`, the node will broadcast to every other node that it is quitting,
and then shut down. Each node will remove it from its node list.

### Reads and writes
When the client sends a `SET` or `GET` message, the first node in the cluster to receive it is
called the "coordinator", as in the Dynamo paper. In this version, `GET`s are handled solely by the
coordinator and returned to the client. When a `SET` occurs, it is synchronously handled
by the coordinator and at least one other node in the cluster before the client receives an `OK`. If
the coordinator cannot reach any other nodes in the cluster, the write is declared failed and the
client is informed. Once the `OK` is sent to the client, the write is then propagated to every
other node in the cluster.

### Assumptions
In this version, it is assumed that:
- the partition count will not be resized after the cluster is created
- new nodes always join and exit the cluster successfully; every existing node
correctly learns about the new node, and the new node has a perfect
map of the cluster which includes itself.
